{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & remove Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_n_rmv_unnamed():\n",
    "    data = pd.read_csv('./extract_seoul.csv')\n",
    "    loc_df = pd.read_csv('./loc_df.csv')\n",
    "    \n",
    "    data = data.drop(['Unnamed: 0'],1)\n",
    "    loc_df = loc_df.drop(['Unnamed: 0'], 1)\n",
    "    \n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    \n",
    "    return data, loc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, loc_df = load_df_n_rmv_unnamed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBAL VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GU_ENG_NAME = ''\n",
    "BASE_PATH = './TimeSeries/'\n",
    "MODEL_NAME = 'LSTM'\n",
    "\n",
    "TRAIN_RATIO = 0.80\n",
    "WINDOW_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dict 관련 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(a, b):\n",
    "    d = dict()\n",
    "    #갯수 다르면\n",
    "    if len(a)-len(b) != 0:\n",
    "        for i in range(len(a)-len(b)):\n",
    "            b.append(None)\n",
    "    #갯수 같으면\n",
    "    for j in range(len(a)):\n",
    "        d[a[j]] = b[j]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_val_using_key(dic, key):\n",
    "    if key in dic:\n",
    "        return dic[key]\n",
    "    else:\n",
    "        print('404 not found')\n",
    "        return dic['서울']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_key_using_val(dic, val):\n",
    "    return [k for k, v in dic.items() if v == val][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loc code gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_code_dict_gen():\n",
    "    loc_code = loc_df['구분']\n",
    "    loc_code_en = loc_df['Classification']\n",
    "    \n",
    "    loc_code_dict = create_dict(loc_code, loc_code_en)\n",
    "    \n",
    "    return loc_code_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_code_dict = loc_code_dict_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'전국'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_key_using_val(loc_code_dict, 'Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_val_using_key(loc_code_dict, '강남aaa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원하는 구만 선택하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_gu_data(df):\n",
    "    gu_name = input()\n",
    "    \n",
    "    global GU_ENG_NAME\n",
    "    search_name = find_val_using_key(loc_code_dict, gu_name)\n",
    "    GU_ENG_NAME = search_name\n",
    "    \n",
    "    return df[['Date', search_name]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관악구\n",
      "Gwanak-gu\n"
     ]
    }
   ],
   "source": [
    "gu_data = select_gu_data(data)\n",
    "print(GU_ENG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (15, 5))\n",
    "ax = sns.lineplot(x=\"Date\", y=GU_ENG_NAME, data=gu_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras &  sklearn import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188 entries, 0 to 187\n",
      "Data columns (total 2 columns):\n",
      "Date         188 non-null datetime64[ns]\n",
      "Gwanak-gu    188 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(1)\n",
      "memory usage: 3.0 KB\n"
     ]
    }
   ],
   "source": [
    "gu_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_date_range(data):\n",
    "    year = input('연도 입력: ')\n",
    "    month = input('월 입력: ')\n",
    "    \n",
    "    date_str = year +'-'+ str(int(month)).zfill(2) + '-01'\n",
    "    \n",
    "    mask = pd.to_datetime(date_str)\n",
    "    \n",
    "    result = data.loc[:][data['Date'] >= mask]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연도 입력: 2010\n",
      "월 입력: 1\n"
     ]
    }
   ],
   "source": [
    "result_dataset = select_date_range(gu_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataset = result_dataset.set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize 0 ~ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset):\n",
    "    scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "    normed = scaler.fit_transform(dataset)\n",
    "    \n",
    "    return normed, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_dataset, scaler = normalize_dataset(result_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = np.array(list(map(lambda x: x+0.0000001, normed_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split tran & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_divide(dataset, train_ratio = 0.8):\n",
    "    train_size = int(len(dataset) * train_ratio)\n",
    "    test_size = len(dataset) - train_size\n",
    "    \n",
    "    train, test = dataset[0:train_size], dataset[train_size:len(dataset)]\n",
    "    print(\"Number of entries (training set, test set): \" + str((len(train), len(test))))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries (training set, test set): (83, 21)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_divide(normed_dataset, TRAIN_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, window_size = 1):\n",
    "    data_X, data_Y = [], []\n",
    "    for i in range(len(dataset) - window_size - 1):\n",
    "        a = dataset[i:(i + window_size), 0]\n",
    "        data_X.append(a)\n",
    "        data_Y.append(dataset[i + window_size, 0])\n",
    "    return(np.array(data_X), np.array(data_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data shape:  (78, 4) (78,)\n",
      "Original test data shape:  (16, 4) (16,)\n",
      "Original whole data shape:  (99, 4) (99,)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y = create_dataset(train, WINDOW_SIZE)\n",
    "test_X, test_Y = create_dataset(test, WINDOW_SIZE)\n",
    "whole_X, whole_Y = create_dataset(normed_dataset, WINDOW_SIZE)\n",
    "\n",
    "print(\"Original training data shape: \", train_X.shape, train_Y.shape)\n",
    "print(\"Original test data shape: \", test_X.shape, test_Y.shape)\n",
    "print(\"Original whole data shape: \", whole_X.shape, whole_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_with_diff(x_last_val, y_val):\n",
    "    diff = np.subtract(y_val, x_last_val)\n",
    "    diff = np.reshape(diff, (diff.shape[0], 1))\n",
    "    \n",
    "    \n",
    "    y_val = np.reshape(y_val, (y_val.shape[0], 1))\n",
    "    \n",
    "    result = np.concatenate((y_val, diff),axis=1)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_y_n_dif = create_dataset_with_diff(train_X[:,3], train_Y)\n",
    "test_y_n_dif = create_dataset_with_diff(test_X[:,3], test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3578087 , -0.00233113],\n",
       "       [ 0.38776495,  0.02995625],\n",
       "       [ 0.5341895 ,  0.14642455],\n",
       "       [ 0.60982881,  0.07563931],\n",
       "       [ 0.78407873,  0.17424992],\n",
       "       [ 0.3963213 , -0.38775742],\n",
       "       [ 0.44575291,  0.0494316 ],\n",
       "       [ 0.44375982, -0.00199308],\n",
       "       [ 0.41224646, -0.03151336],\n",
       "       [ 0.48658106,  0.0743346 ],\n",
       "       [ 0.47622145, -0.01035961],\n",
       "       [ 0.55701151,  0.08079006],\n",
       "       [ 0.5870038 ,  0.02999229],\n",
       "       [ 0.48835732, -0.09864647],\n",
       "       [ 0.6264669 ,  0.13810958],\n",
       "       [ 0.70510776,  0.07864086]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_n_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dim = len(train_y_n_dif[0])\n",
    "output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_dataset(train_x, test_x, whole_x):\n",
    "    train_x = np.reshape(train_x, (train_x.shape[0], 1, train_x.shape[1]))\n",
    "    test_x = np.reshape(test_x, (test_x.shape[0], 1, test_x.shape[1]))\n",
    "    whole_x = np.reshape(whole_x ,(whole_x.shape[0], 1, whole_x.shape[1]))\n",
    "    \n",
    "    print(\"train_X re-shape: \", train_x.shape)\n",
    "    print(\"test_X re-shape: \", test_x.shape)\n",
    "    print(\"whole_X re-shape: \", whole_x.shape)\n",
    "    return train_x, test_x, whole_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X re-shape:  (78, 1, 4)\n",
      "test_X re-shape:  (16, 1, 4)\n",
      "whole_X re-shape:  (99, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, whole_X = reshape_dataset(train_X, test_X, whole_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = ''\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    os.mkdir(BASE_PATH)\n",
    "\n",
    "def create_checkpoint(model_name):\n",
    "    global MODEL_PATH\n",
    "    MODEL_PATH = os.path.join(BASE_PATH, model_name)\n",
    "    \n",
    "    \n",
    "    MODEL_PATH = MODEL_PATH + \"/\" + GU_ENG_NAME\n",
    "    print(\"Model_path : \", MODEL_PATH)\n",
    "    \n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.mkdir(MODEL_PATH)\n",
    "    \n",
    "    return ModelCheckpoint(filepath=os.path.join(MODEL_PATH, 'val_loss-{val_loss:.6f}.hdf5'),\n",
    "                           monitor='val_loss',\n",
    "                           verbose=2,\n",
    "                           save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=80,  verbose=2, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(train_X, train_Y, test_X, test_Y, window_size = 1):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM (256,  activation = 'relu', return_sequences = False ,inner_activation = 'hard_sigmoid' , bias_regularizer=L1L2(l1=0.01, l2=0.01),  input_shape =(1, window_size)))\n",
    "    model.add(Dense(output_dim))\n",
    "    \n",
    "#     adam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=1e-6, amsgrad=False)\n",
    "    adam = Adam(lr=0.001, beta_1=0.89, beta_2=0.999, epsilon=1e-8, decay=1e-6, amsgrad=True)\n",
    "    \n",
    "    checkloss = create_checkpoint(MODEL_NAME)\n",
    "    \n",
    "    model.compile(loss = \"mean_squared_error\", optimizer = adam)\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit(train_X, train_Y, epochs = 2000 ,batch_size=4, validation_data=(test_X, test_Y),\n",
    "                        shuffle=False,\n",
    "                        callbacks=[checkloss, early_stopping])\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tax_k/anaconda3/envs/DL/lib/python3.6/site-packages/ipykernel/__main__.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(256, activation=\"relu\", return_sequences=False, bias_regularizer=<keras.reg..., input_shape=(1, 4), recurrent_activation=\"hard_sigmoid\")`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_path :  ./TimeSeries/LSTM/Gwanak-gu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 256)               267264    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 267,778\n",
      "Trainable params: 267,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 78 samples, validate on 16 samples\n",
      "Epoch 1/2000\n",
      "78/78 [==============================] - 2s 30ms/step - loss: 5.0974 - val_loss: 5.0504\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.05037, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-5.050369.hdf5\n",
      "Epoch 2/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 4.9196 - val_loss: 4.8575\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.05037 to 4.85754, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-4.857536.hdf5\n",
      "Epoch 3/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 4.7571 - val_loss: 4.6914\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.85754 to 4.69141, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-4.691411.hdf5\n",
      "Epoch 4/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 4.6088 - val_loss: 4.5450\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.69141 to 4.54500, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-4.544997.hdf5\n",
      "Epoch 5/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 4.4662 - val_loss: 4.4043\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.54500 to 4.40427, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-4.404272.hdf5\n",
      "Epoch 6/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 4.3266 - val_loss: 4.2663\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.40427 to 4.26628, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-4.266278.hdf5\n",
      "Epoch 7/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 4.1898 - val_loss: 4.1308\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.26628 to 4.13082, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-4.130818.hdf5\n",
      "Epoch 8/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 4.0557 - val_loss: 3.9983\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.13082 to 3.99835, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-3.998349.hdf5\n",
      "Epoch 9/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 3.9246 - val_loss: 3.8687\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.99835 to 3.86870, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-3.868698.hdf5\n",
      "Epoch 10/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 3.7963 - val_loss: 3.7417\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.86870 to 3.74170, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-3.741704.hdf5\n",
      "Epoch 11/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 3.6706 - val_loss: 3.6175\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.74170 to 3.61748, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-3.617482.hdf5\n",
      "Epoch 12/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 3.5477 - val_loss: 3.4959\n",
      "\n",
      "Epoch 00012: val_loss improved from 3.61748 to 3.49593, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-3.495928.hdf5\n",
      "Epoch 13/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 3.4274 - val_loss: 3.3771\n",
      "\n",
      "Epoch 00013: val_loss improved from 3.49593 to 3.37706, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-3.377056.hdf5\n",
      "Epoch 14/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 3.3098 - val_loss: 3.2607\n",
      "\n",
      "Epoch 00014: val_loss improved from 3.37706 to 3.26069, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-3.260693.hdf5\n",
      "Epoch 15/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 3.1947 - val_loss: 3.1469\n",
      "\n",
      "Epoch 00015: val_loss improved from 3.26069 to 3.14695, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-3.146949.hdf5\n",
      "Epoch 16/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 3.0821 - val_loss: 3.0357\n",
      "\n",
      "Epoch 00016: val_loss improved from 3.14695 to 3.03566, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-3.035656.hdf5\n",
      "Epoch 17/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 2.9721 - val_loss: 2.9269\n",
      "\n",
      "Epoch 00017: val_loss improved from 3.03566 to 2.92690, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-2.926897.hdf5\n",
      "Epoch 18/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 2.8645 - val_loss: 2.8206\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.92690 to 2.82058, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-2.820582.hdf5\n",
      "Epoch 19/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 2.7594 - val_loss: 2.7166\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.82058 to 2.71665, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-2.716648.hdf5\n",
      "Epoch 20/2000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 2.6567 - val_loss: 2.6152\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.71665 to 2.61521, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-2.615205.hdf5\n",
      "Epoch 21/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 2.5563 - val_loss: 2.5160\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.61521 to 2.51604, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-2.516038.hdf5\n",
      "Epoch 22/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 2.4583 - val_loss: 2.4191\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.51604 to 2.41908, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-2.419083.hdf5\n",
      "Epoch 23/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 2.3625 - val_loss: 2.3245\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.41908 to 2.32451, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-2.324513.hdf5\n",
      "Epoch 24/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 2.2690 - val_loss: 2.2322\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.32451 to 2.23215, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-2.232152.hdf5\n",
      "Epoch 25/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 2.1776 - val_loss: 2.1419\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.23215 to 2.14193, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-2.141926.hdf5\n",
      "Epoch 26/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 2.0885 - val_loss: 2.0539\n",
      "\n",
      "Epoch 00026: val_loss improved from 2.14193 to 2.05394, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-2.053940.hdf5\n",
      "Epoch 27/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 2.0015 - val_loss: 1.9681\n",
      "\n",
      "Epoch 00027: val_loss improved from 2.05394 to 1.96810, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-1.968102.hdf5\n",
      "Epoch 28/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 1.9167 - val_loss: 1.8842\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.96810 to 1.88424, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-1.884236.hdf5\n",
      "Epoch 29/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 1.8339 - val_loss: 1.8026\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.88424 to 1.80256, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-1.802556.hdf5\n",
      "Epoch 30/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 1.7532 - val_loss: 1.7229\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.80256 to 1.72291, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-1.722914.hdf5\n",
      "Epoch 31/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 1.6744 - val_loss: 1.6452\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.72291 to 1.64518, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-1.645175.hdf5\n",
      "Epoch 32/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 1.5977 - val_loss: 1.5695\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.64518 to 1.56950, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-1.569499.hdf5\n",
      "Epoch 33/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 1.5229 - val_loss: 1.4957\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.56950 to 1.49573, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-1.495729.hdf5\n",
      "Epoch 34/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 1.4500 - val_loss: 1.4238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: val_loss improved from 1.49573 to 1.42379, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-1.423787.hdf5\n",
      "Epoch 35/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 1.3790 - val_loss: 1.3539\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.42379 to 1.35385, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-1.353853.hdf5\n",
      "Epoch 36/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 1.3099 - val_loss: 1.2857\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.35385 to 1.28567, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-1.285666.hdf5\n",
      "Epoch 37/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 1.2426 - val_loss: 1.2193\n",
      "\n",
      "Epoch 00037: val_loss improved from 1.28567 to 1.21929, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-1.219293.hdf5\n",
      "Epoch 38/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 1.1771 - val_loss: 1.1547\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.21929 to 1.15473, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-1.154726.hdf5\n",
      "Epoch 39/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 1.1133 - val_loss: 1.0919\n",
      "\n",
      "Epoch 00039: val_loss improved from 1.15473 to 1.09188, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-1.091875.hdf5\n",
      "Epoch 40/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 1.0512 - val_loss: 1.0307\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.09188 to 1.03074, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-1.030742.hdf5\n",
      "Epoch 41/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9909 - val_loss: 0.9714\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.03074 to 0.97135, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.971350.hdf5\n",
      "Epoch 42/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9322 - val_loss: 0.9136\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.97135 to 0.91364, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.913636.hdf5\n",
      "Epoch 43/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.8753 - val_loss: 0.8575\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.91364 to 0.85754, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.857538.hdf5\n",
      "Epoch 44/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.8200 - val_loss: 0.8031\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.85754 to 0.80313, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.803130.hdf5\n",
      "Epoch 45/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.7663 - val_loss: 0.7504\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.80313 to 0.75040, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.750401.hdf5\n",
      "Epoch 46/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.7143 - val_loss: 0.6992\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.75040 to 0.69921, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.699212.hdf5\n",
      "Epoch 47/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.6638 - val_loss: 0.6496\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.69921 to 0.64964, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.649635.hdf5\n",
      "Epoch 48/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.6150 - val_loss: 0.6016\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.64964 to 0.60161, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.601611.hdf5\n",
      "Epoch 49/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.5676 - val_loss: 0.5552\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.60161 to 0.55515, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.555153.hdf5\n",
      "Epoch 50/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.5218 - val_loss: 0.5101\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.55515 to 0.51014, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.510144.hdf5\n",
      "Epoch 51/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.4775 - val_loss: 0.4667\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.51014 to 0.46665, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.466653.hdf5\n",
      "Epoch 52/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.4346 - val_loss: 0.4245\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.46665 to 0.42454, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.424542.hdf5\n",
      "Epoch 53/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.3932 - val_loss: 0.3839\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.42454 to 0.38390, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.383898.hdf5\n",
      "Epoch 54/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.3531 - val_loss: 0.3445\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.38390 to 0.34455, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.344546.hdf5\n",
      "Epoch 55/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.3144 - val_loss: 0.3067\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.34455 to 0.30666, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.306661.hdf5\n",
      "Epoch 56/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.2771 - val_loss: 0.2700\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.30666 to 0.26997, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.269974.hdf5\n",
      "Epoch 57/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.2410 - val_loss: 0.2347\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.26997 to 0.23465, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.234654.hdf5\n",
      "Epoch 58/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.2062 - val_loss: 0.2005\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.23465 to 0.20048, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.200480.hdf5\n",
      "Epoch 59/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.1726 - val_loss: 0.1676\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.20048 to 0.16759, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.167589.hdf5\n",
      "Epoch 60/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.1402 - val_loss: 0.1359\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.16759 to 0.13586, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.135856.hdf5\n",
      "Epoch 61/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.1090 - val_loss: 0.1053\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.13586 to 0.10529, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.105288.hdf5\n",
      "Epoch 62/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0789 - val_loss: 0.0758\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.10529 to 0.07576, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.075756.hdf5\n",
      "Epoch 63/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.0474\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.07576 to 0.04740, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.047401.hdf5\n",
      "Epoch 64/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0200\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.04740 to 0.02001, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.020011.hdf5\n",
      "Epoch 65/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0175\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.02001 to 0.01747, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.017469.hdf5\n",
      "Epoch 66/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0042 - val_loss: 0.0168\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.01747 to 0.01675, saving model to ./TimeSeries/LSTM/Gwanak-gu/val_loss-0.016752.hdf5\n",
      "Epoch 67/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0041 - val_loss: 0.0168\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.01675\n",
      "Epoch 68/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0041 - val_loss: 0.0169\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.01675\n",
      "Epoch 69/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0041 - val_loss: 0.0169\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.01675\n",
      "Epoch 70/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0041 - val_loss: 0.0169\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.01675\n",
      "Epoch 71/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0170\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.01675\n",
      "Epoch 72/2000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0041 - val_loss: 0.0170\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.01675\n",
      "Epoch 73/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0171\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.01675\n",
      "Epoch 74/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0171\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.01675\n",
      "Epoch 75/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0171\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.01675\n",
      "Epoch 76/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0041 - val_loss: 0.0171\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.01675\n",
      "Epoch 77/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.01675\n",
      "Epoch 78/2000\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0171\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.01675\n",
      "Epoch 79/2000\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0173\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.01675\n",
      "Epoch 80/2000\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.01675\n",
      "Epoch 81/2000\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0173\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.01675\n",
      "Epoch 82/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0172\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.01675\n",
      "Epoch 83/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0174\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.01675\n",
      "Epoch 84/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0173\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.01675\n",
      "Epoch 85/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0175\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.01675\n",
      "Epoch 86/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0175\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.01675\n",
      "Epoch 87/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0175\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.01675\n",
      "Epoch 88/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0176\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.01675\n",
      "Epoch 89/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0176\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.01675\n",
      "Epoch 90/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0177\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.01675\n",
      "Epoch 91/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0176\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.01675\n",
      "Epoch 92/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0177\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.01675\n",
      "Epoch 93/2000\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.0178\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.01675\n",
      "Epoch 94/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0178\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.01675\n",
      "Epoch 95/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0179\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01675\n",
      "Epoch 96/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0178\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01675\n",
      "Epoch 97/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0179\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01675\n",
      "Epoch 98/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0180\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.01675\n",
      "Epoch 99/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0180\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01675\n",
      "Epoch 100/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0181\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.01675\n",
      "Epoch 101/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0182\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.01675\n",
      "Epoch 102/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0182\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.01675\n",
      "Epoch 103/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0182\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.01675\n",
      "Epoch 104/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0183\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.01675\n",
      "Epoch 105/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0184\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.01675\n",
      "Epoch 106/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0184\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.01675\n",
      "Epoch 107/2000\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.0184\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.01675\n",
      "Epoch 108/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0184\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.01675\n",
      "Epoch 109/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0186\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.01675\n",
      "Epoch 110/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0185\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.01675\n",
      "Epoch 111/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0187\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.01675\n",
      "Epoch 112/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0187\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.01675\n",
      "Epoch 113/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0188\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.01675\n",
      "Epoch 114/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0188\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.01675\n",
      "Epoch 115/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0040 - val_loss: 0.0189\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.01675\n",
      "Epoch 116/2000\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0040 - val_loss: 0.0190\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.01675\n",
      "Epoch 117/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0040 - val_loss: 0.0190\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.01675\n",
      "Epoch 118/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0191\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.01675\n",
      "Epoch 119/2000\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0040 - val_loss: 0.0192\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.01675\n",
      "Epoch 120/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0193\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.01675\n",
      "Epoch 121/2000\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.0192\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.01675\n",
      "Epoch 122/2000\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.0193\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.01675\n",
      "Epoch 123/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0193\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.01675\n",
      "Epoch 124/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0194\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.01675\n",
      "Epoch 125/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0195\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.01675\n",
      "Epoch 126/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - ETA: 0s - loss: 0.003 - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0196\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.01675\n",
      "Epoch 127/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0196\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.01675\n",
      "Epoch 128/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0197\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.01675\n",
      "Epoch 129/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0198\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.01675\n",
      "Epoch 130/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0198\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.01675\n",
      "Epoch 131/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0199\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.01675\n",
      "Epoch 132/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0198\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.01675\n",
      "Epoch 133/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0199\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.01675\n",
      "Epoch 134/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0200\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.01675\n",
      "Epoch 135/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0201\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.01675\n",
      "Epoch 136/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0202\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.01675\n",
      "Epoch 137/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0203\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.01675\n",
      "Epoch 138/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0203\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.01675\n",
      "Epoch 139/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0205\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.01675\n",
      "Epoch 140/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0205\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.01675\n",
      "Epoch 141/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0205\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.01675\n",
      "Epoch 142/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0206\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.01675\n",
      "Epoch 143/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0206\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.01675\n",
      "Epoch 144/2000\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.0209\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.01675\n",
      "Epoch 145/2000\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.0208\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.01675\n",
      "Epoch 146/2000\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0208\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.01675\n",
      "Epoch 00146: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = Model(train_X, train_y_n_dif, test_X, test_y_n_dif, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_score2(model, X, Y):\n",
    "    pred = scaler.inverse_transform(model.predict(X))\n",
    "    gt_val = scaler.inverse_transform(Y)\n",
    "    \n",
    "    return (pred, gt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.17167185, -0.9592216 ],\n",
       "        [-0.24710111, -0.93677425],\n",
       "        [-0.34316722, -0.95818925],\n",
       "        [-0.2885273 , -1.0059854 ],\n",
       "        [-0.2229434 , -1.006612  ],\n",
       "        [-0.08682556, -1.0077233 ],\n",
       "        [-0.01154583, -1.0113946 ],\n",
       "        [-0.05964732, -1.0045253 ],\n",
       "        [-0.12329067, -0.98333883],\n",
       "        [ 0.002339  , -1.0463797 ],\n",
       "        [ 0.2529733 , -1.0460112 ],\n",
       "        [ 0.18552527, -0.9865821 ],\n",
       "        [ 0.13910522, -1.0356908 ],\n",
       "        [-0.02775462, -0.9006266 ],\n",
       "        [ 0.01759779, -1.0446211 ],\n",
       "        [-0.11849546, -0.8990381 ],\n",
       "        [-0.06266732, -1.057931  ],\n",
       "        [ 0.00407828, -0.98980975],\n",
       "        [-0.06705586, -1.0100505 ],\n",
       "        [-0.04155639, -1.0119866 ],\n",
       "        [-0.11631028, -0.9687122 ],\n",
       "        [-0.07830474, -1.028038  ],\n",
       "        [-0.04498279, -1.0023425 ],\n",
       "        [-0.07277604, -1.0110704 ],\n",
       "        [-0.29743868, -0.8683551 ],\n",
       "        [-0.22911045, -1.0345712 ],\n",
       "        [-0.14905874, -0.9858468 ],\n",
       "        [-0.2755593 , -0.9642108 ],\n",
       "        [-0.2553662 , -1.0067165 ],\n",
       "        [-0.28587785, -0.9674698 ],\n",
       "        [-0.6898881 , -0.71368396],\n",
       "        [-0.52086246, -0.9076218 ],\n",
       "        [-0.37017053, -0.8674498 ],\n",
       "        [-0.2772416 , -0.87643474],\n",
       "        [-0.07757232, -0.9457704 ],\n",
       "        [-0.04798042, -0.94885254],\n",
       "        [-0.14101753, -1.0088096 ],\n",
       "        [-0.05361523, -1.0294906 ],\n",
       "        [-0.0076156 , -1.0158226 ],\n",
       "        [-0.15389344, -0.9401275 ],\n",
       "        [-0.16652463, -1.0099795 ],\n",
       "        [-0.05854213, -1.0184101 ],\n",
       "        [ 0.11307165, -1.0298817 ],\n",
       "        [ 0.11738069, -1.0173901 ],\n",
       "        [ 0.06108527, -1.0117233 ],\n",
       "        [ 0.02535621, -0.99502563],\n",
       "        [ 0.10150749, -1.0384047 ],\n",
       "        [ 0.20337671, -1.0423768 ],\n",
       "        [ 0.13216387, -0.9952392 ],\n",
       "        [ 0.01245616, -0.9706606 ],\n",
       "        [-0.05649716, -0.9738739 ],\n",
       "        [ 0.02272674, -1.030683  ],\n",
       "        [ 0.03406249, -1.0055233 ],\n",
       "        [ 0.14972994, -1.0660866 ],\n",
       "        [ 0.11909131, -0.9877245 ],\n",
       "        [ 0.05829412, -1.0181464 ],\n",
       "        [-0.02418394, -0.9574612 ],\n",
       "        [-0.02779419, -1.0172828 ],\n",
       "        [ 0.04874885, -1.0215672 ],\n",
       "        [ 0.322048  , -1.049702  ],\n",
       "        [ 0.43420404, -1.0014626 ],\n",
       "        [ 0.51637125, -1.0575317 ],\n",
       "        [ 0.41259193, -0.981629  ],\n",
       "        [ 0.5910618 , -1.1201006 ],\n",
       "        [ 0.6125682 , -1.0060999 ],\n",
       "        [ 0.6014629 , -1.0796733 ],\n",
       "        [ 0.4081411 , -0.89710605],\n",
       "        [ 0.49418798, -1.0896986 ],\n",
       "        [ 0.3617112 , -0.9105003 ],\n",
       "        [ 0.17813209, -0.98817   ],\n",
       "        [ 0.19347562, -0.99600035],\n",
       "        [ 0.16583277, -1.0086629 ],\n",
       "        [ 0.16074614, -1.0216136 ],\n",
       "        [ 0.20975833, -1.0349246 ],\n",
       "        [ 0.25341946, -1.0360787 ],\n",
       "        [ 0.48234707, -1.094052  ],\n",
       "        [ 0.52045465, -1.0384294 ],\n",
       "        [ 0.3986172 , -1.0068829 ]], dtype=float32),\n",
       " array([[-3.31759176e-01, -1.13341208e+00],\n",
       "        [-4.15703209e-01, -1.09343442e+00],\n",
       "        [-3.04406077e-01, -8.98193252e-01],\n",
       "        [-2.40562991e-01, -9.45647298e-01],\n",
       "        [-8.08429016e-02, -8.49770295e-01],\n",
       "        [-1.62198264e-02, -9.44867309e-01],\n",
       "        [-7.71043867e-02, -1.07037494e+00],\n",
       "        [-1.57462983e-01, -1.08984898e+00],\n",
       "        [ 4.31220567e-02, -8.08905345e-01],\n",
       "        [ 2.93422908e-01, -7.59189533e-01],\n",
       "        [ 1.51017418e-01, -1.15189587e+00],\n",
       "        [ 1.61641350e-01, -9.98866453e-01],\n",
       "        [-1.17774293e-01, -1.28890603e+00],\n",
       "        [ 5.98831930e-02, -8.31832898e-01],\n",
       "        [-2.32069351e-01, -1.30144293e+00],\n",
       "        [-3.88079388e-04, -7.77809112e-01],\n",
       "        [-3.34072912e-02, -1.04250960e+00],\n",
       "        [-7.46007943e-02, -1.05068389e+00],\n",
       "        [-4.73047119e-02, -9.82194302e-01],\n",
       "        [-1.68224784e-01, -1.13041046e+00],\n",
       "        [-6.22351434e-02, -9.03500744e-01],\n",
       "        [-6.60807582e-02, -1.01333600e+00],\n",
       "        [-8.32847910e-02, -1.02669442e+00],\n",
       "        [-4.24680336e-01, -1.35088593e+00],\n",
       "        [-1.99349358e-01, -7.84159406e-01],\n",
       "        [-1.92684263e-01, -1.00282529e+00],\n",
       "        [-3.32893079e-01, -1.14969920e+00],\n",
       "        [-2.70598109e-01, -9.47195414e-01],\n",
       "        [-3.54483978e-01, -1.09337625e+00],\n",
       "        [-1.00949038e+00, -1.66449679e+00],\n",
       "        [-6.20720898e-01, -6.20720898e-01],\n",
       "        [-5.06325539e-01, -8.95095026e-01],\n",
       "        [-3.56034543e-01, -8.59199388e-01],\n",
       "        [-9.34059972e-02, -7.46861838e-01],\n",
       "        [-1.15073705e-01, -1.03115809e+00],\n",
       "        [-1.52400000e-01, -1.04681668e+00],\n",
       "        [-3.67021963e-02, -8.93792580e-01],\n",
       "        [-1.26359402e-02, -9.85424128e-01],\n",
       "        [-2.22825621e-01, -1.21968007e+00],\n",
       "        [-1.72842902e-01, -9.59507665e-01],\n",
       "        [-5.50148849e-02, -8.91662367e-01],\n",
       "        [ 1.40163878e-01, -8.14311621e-01],\n",
       "        [ 1.10579284e-01, -1.03907498e+00],\n",
       "        [ 5.78475651e-02, -1.06222210e+00],\n",
       "        [ 4.81091698e-03, -1.06252703e+00],\n",
       "        [ 1.29202596e-01, -8.85098705e-01],\n",
       "        [ 2.28429027e-01, -9.10263953e-01],\n",
       "        [ 1.10345873e-01, -1.12757354e+00],\n",
       "        [-1.89815017e-02, -1.13881776e+00],\n",
       "        [-9.38736605e-02, -1.08438254e+00],\n",
       "        [ 4.37284068e-02, -8.71888317e-01],\n",
       "        [ 1.56394190e-02, -1.03757937e+00],\n",
       "        [ 2.09494373e-01, -8.15635430e-01],\n",
       "        [ 8.33802139e-02, -1.13560454e+00],\n",
       "        [ 6.44618397e-02, -1.02840876e+00],\n",
       "        [-7.80559423e-02, -1.15200817e+00],\n",
       "        [-2.35337318e-02, -9.54968174e-01],\n",
       "        [ 5.44356742e-02, -9.31520978e-01],\n",
       "        [ 3.81407595e-01, -6.82518464e-01],\n",
       "        [ 4.10092583e-01, -9.80805396e-01],\n",
       "        [ 5.51848210e-01, -8.67734757e-01],\n",
       "        [ 3.78994746e-01, -1.18234385e+00],\n",
       "        [ 6.87511778e-01, -7.00973352e-01],\n",
       "        [ 5.77509195e-01, -1.11949297e+00],\n",
       "        [ 6.49915407e-01, -9.37084172e-01],\n",
       "        [ 3.32180619e-01, -1.32722517e+00],\n",
       "        [ 5.72430539e-01, -7.69240464e-01],\n",
       "        [ 2.71986664e-01, -1.30993426e+00],\n",
       "        [ 1.79467409e-01, -1.10200964e+00],\n",
       "        [ 1.78985435e-01, -1.00997236e+00],\n",
       "        [ 1.56667460e-01, -1.03180836e+00],\n",
       "        [ 1.65574456e-01, -1.00058339e+00],\n",
       "        [ 2.28459564e-01, -9.46605276e-01],\n",
       "        [ 2.69877484e-01, -9.68072464e-01],\n",
       "        [ 5.60011294e-01, -7.19356575e-01],\n",
       "        [ 5.19081557e-01, -1.05042012e+00],\n",
       "        [ 3.97708026e-01, -1.13086392e+00],\n",
       "        [ 5.05653063e-01, -9.01545347e-01]]))"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_and_score2(model[0], train_X, train_y_n_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict, train_orig = predict_and_score2(model[0], train_X, train_y_n_dif)\n",
    "test_predict, test_orig = predict_and_score2(model[0], test_X, test_y_n_dif)\n",
    "# whole_test,mae_whole, whole_predict, whole_orig = predict_and_score(model[0], whole_X, whole_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_predict_n_plot(dataset, train_predict, test_predict):\n",
    "    train_predict_plot = np.empty_like(dataset)\n",
    "    train_predict_plot[:, :] = np.nan\n",
    "    \n",
    "    t_prd = np.array(train_predict[:,0])\n",
    "    t_prd = np.reshape(t_prd, (t_prd.shape[0], 1))\n",
    "    \n",
    "    train_predict_plot[WINDOW_SIZE - 1:len(train_predict) + WINDOW_SIZE - 1, :] = t_prd\n",
    "\n",
    "    test_predict_plot = np.empty_like(dataset)\n",
    "    test_predict_plot[:, :] = np.nan\n",
    "    \n",
    "    test_prd = np.array(test_predict[:,0])\n",
    "    test_prd = np.reshape(test_prd, (test_prd.shape[0], 1))\n",
    "    \n",
    "    test_predict_plot[len(test_predict) + (WINDOW_SIZE * 2) :len(dataset) - 2 , :] = t_prd\n",
    "    \n",
    "    print(test_predict_plot)\n",
    "    \n",
    "    title_label = GU_ENG_NAME + \" Train / Test Predict\"\n",
    "\n",
    "    plt.figure(figsize = (15, 5))\n",
    "    plt.plot(dataset, label = \"True value\")\n",
    "    plt.plot(train_predict_plot, label = \"Training set pred\")\n",
    "#     plt.plot(test_predict_plot, label = \"Test set pred\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.title(title_label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.17167185]\n",
      " [-0.24710111]\n",
      " [-0.34316722]\n",
      " [-0.28852731]\n",
      " [-0.2229434 ]\n",
      " [-0.08682556]\n",
      " [-0.01154583]\n",
      " [-0.05964732]\n",
      " [-0.12329067]\n",
      " [ 0.002339  ]\n",
      " [ 0.25297329]\n",
      " [ 0.18552527]\n",
      " [ 0.13910522]\n",
      " [-0.02775462]\n",
      " [ 0.01759779]\n",
      " [-0.11849546]\n",
      " [-0.06266732]\n",
      " [ 0.00407828]\n",
      " [-0.06705586]\n",
      " [-0.04155639]\n",
      " [-0.11631028]\n",
      " [-0.07830474]\n",
      " [-0.04498279]\n",
      " [-0.07277604]\n",
      " [-0.29743868]\n",
      " [-0.22911045]\n",
      " [-0.14905874]\n",
      " [-0.27555931]\n",
      " [-0.25536621]\n",
      " [-0.28587785]\n",
      " [-0.68988812]\n",
      " [-0.52086246]\n",
      " [-0.37017053]\n",
      " [-0.27724159]\n",
      " [-0.07757232]\n",
      " [-0.04798042]\n",
      " [-0.14101753]\n",
      " [-0.05361523]\n",
      " [-0.0076156 ]\n",
      " [-0.15389344]\n",
      " [-0.16652463]\n",
      " [-0.05854213]\n",
      " [ 0.11307165]\n",
      " [ 0.11738069]\n",
      " [ 0.06108527]\n",
      " [ 0.02535621]\n",
      " [ 0.10150749]\n",
      " [ 0.20337671]\n",
      " [ 0.13216387]\n",
      " [ 0.01245616]\n",
      " [-0.05649716]\n",
      " [ 0.02272674]\n",
      " [ 0.03406249]\n",
      " [ 0.14972994]\n",
      " [ 0.11909131]\n",
      " [ 0.05829412]\n",
      " [-0.02418394]\n",
      " [-0.02779419]\n",
      " [ 0.04874885]\n",
      " [ 0.32204801]\n",
      " [ 0.43420404]\n",
      " [ 0.51637125]\n",
      " [ 0.41259193]\n",
      " [ 0.59106177]\n",
      " [ 0.6125682 ]\n",
      " [ 0.6014629 ]\n",
      " [ 0.40814111]\n",
      " [ 0.49418798]\n",
      " [ 0.3617112 ]\n",
      " [ 0.17813209]\n",
      " [ 0.19347562]\n",
      " [ 0.16583277]\n",
      " [ 0.16074614]\n",
      " [ 0.20975833]\n",
      " [ 0.25341946]\n",
      " [ 0.48234707]\n",
      " [ 0.52045465]\n",
      " [ 0.39861721]\n",
      " [        nan]\n",
      " [        nan]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "view limit minimum -36842.200000000004 is less than 1 and is an invalid Matplotlib date value. This often happens if you pass a non-datetime value to an axis that has datetime units",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-275-7f03b3c09788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart_predict_n_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-274-9928208da8a9>\u001b[0m in \u001b[0;36mstart_predict_n_plot\u001b[0;34m(dataset, train_predict, test_predict)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \"\"\"\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     37\u001b[0m             display(\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             )\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36m_fetch_figure_metadata\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m# the background is transparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         ticksLight = _is_light([label.get_color()\n\u001b[0;32m--> 177\u001b[0;31m                                 \u001b[0;32mfor\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m                                 \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                                 for label in axis.get_ticklabels()])\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    177\u001b[0m                                 \u001b[0;32mfor\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                                 \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                                 for label in axis.get_ticklabels()])\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mticksLight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mticksLight\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mticksLight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;31m# there are one or more tick labels, all with the same lightness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_ticklabels\u001b[0;34m(self, minor, which)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mminor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_minorticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_majorticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_majorticklines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_majorticklabels\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_majorticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;34m'Return a list of Text instances for the major ticklabels'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m         \u001b[0mticks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_major_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m         \u001b[0mlabels1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mticks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel1On\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0mlabels2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mticks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel2On\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_major_ticks\u001b[0;34m(self, numticks)\u001b[0m\n\u001b[1;32m   1389\u001b[0m         \u001b[0;34m'get the tick instances; grow as necessary'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumticks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1391\u001b[0;31m             \u001b[0mnumticks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_major_locator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnumticks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/matplotlib/dates.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;34m'Return the locations of the ticks'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_locator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/matplotlib/dates.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0;34m'Refresh internal information based on current limits.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0mdmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewlim_to_dt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_locator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_locator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.6/site-packages/matplotlib/dates.py\u001b[0m in \u001b[0;36mviewlim_to_dt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    998\u001b[0m                              \u001b[0;34m'often happens if you pass a non-datetime '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m                              \u001b[0;34m'value to an axis that has datetime units'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m                              .format(vmin))\n\u001b[0m\u001b[1;32m   1001\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnum2date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum2date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: view limit minimum -36842.200000000004 is less than 1 and is an invalid Matplotlib date value. This often happens if you pass a non-datetime value to an axis that has datetime units"
     ]
    }
   ],
   "source": [
    "start_predict_n_plot(result_dataset, train_predict, test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_score(model, X, Y):\n",
    "    pred = scaler.inverse_transform(model.predict(X))\n",
    "    # 0 ~ 1 inverse origin\n",
    "    gt_val = scaler.inverse_transform([Y])\n",
    "    \n",
    "    print(len(gt_val[0]))\n",
    "    print(len(pred[:, 0]))\n",
    "    \n",
    "    #RMSE.\n",
    "    rmse = math.sqrt(mean_squared_error(gt_val[0], pred[:, 0]))\n",
    "    \n",
    "    #MAE\n",
    "    mae = mean_absolute_error(gt_val[0], pred[:, 0])\n",
    "    \n",
    "    return(rmse, mae, pred, gt_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_predict[-window_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(model, X, window_size):\n",
    "    \n",
    "    trans = X[-window_size:].T\n",
    "    \n",
    "    \n",
    "    tmp_shape = np.reshape(trans, (trans.shape[0], 1, trans.shape[1]))\n",
    "    pred = model.predict(tmp_shape)\n",
    "    print(\"pred: \", pred)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_pred = predict_future(model[0], whole_predict, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_future_data(model, predict, nb_future):\n",
    "    for i in range(nb_future):\n",
    "        print(\"len before: \", len(predict))\n",
    "        future_pred = predict_future(model, predict, WINDOW_SIZE)\n",
    "        predict = np.append(predict, future_pred, axis=0)\n",
    "        \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_pred_data = make_future_data(model[0] ,whole_predict, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_predict_n_plot(dataset, predict):\n",
    "    \n",
    "    true_value_plot = np.empty_like(dataset)\n",
    "    print(true_value_plot.shape)\n",
    "    print(whole_predict.shape)\n",
    "    \n",
    "    print('dataset_len : ', len(dataset))\n",
    "    print('whole_predict_len : ', len(predict))\n",
    "    \n",
    "    \n",
    "#     test_predict_plot[:, :] = np.nan\n",
    "#     test_predict_plot[(window_size - 1): len(whole_predict) + (window_size - 1), :] = whole_predict\n",
    "\n",
    "    whole_predict_plot = np.empty_like(predict)\n",
    "    whole_predict_plot[:, :] = predict\n",
    "    \n",
    "    for i in range(WINDOW_SIZE - 1):\n",
    "        whole_predict_plot = np.insert(whole_predict_plot, 0, 0)\n",
    "    \n",
    "    title_label = GU_ENG_NAME + \" Train / Test Predict\"\n",
    "    \n",
    "    plt.figure(figsize = (15, 5))\n",
    "    plt.plot(scaler.inverse_transform(dataset), label = \"True value\")\n",
    "    plt.plot(whole_predict_plot, label = \"Test set pred\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.title(title_label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_predict_n_plot(dataset, after_pred_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_dataset(normed_dataset, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in tscv.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(\"\\n\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    model = Model(X_train, y_train, X_test, y_test, window_size)\n",
    "    \n",
    "    best_weights = get_best_weights_path()\n",
    "    model[0].load_weights(best_weights)\n",
    "    \n",
    "    plt.figure(figsize = (15, 5))\n",
    "    plt.plot(model[1].history['loss'], label='train')\n",
    "    plt.plot(model[1].history['val_loss'], label='test')\n",
    "    plt.ylim([0.0, 0.05])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    reshaped_X = np.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
    "    pred = scaler.inverse_transform(model[0].predict(reshaped_X))\n",
    "    orig_data = scaler.inverse_transform([y])\n",
    "    \n",
    "    plt.figure(figsize = (15, 5))\n",
    "    plt.plot(scaler.inverse_transform(dataset), label = \"True value\")\n",
    "    plt.plot(pred[:, 0], label = \"Test set pred\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.title(\"Gangnam Train/test Predict\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    whole_predict = pred[:, 0]\n",
    "    for i in range(10):\n",
    "        future_pred = predict_future(model[0], whole_predict, window_size)\n",
    "        whole_predict.shape = (whole_predict.size//1, 1)\n",
    "        whole_predict = np.append(whole_predict, future_pred, axis=0)\n",
    "        \n",
    "    plt.figure(figsize = (15, 5))\n",
    "    plt.plot(scaler.inverse_transform(dataset), label = \"True value\")\n",
    "    plt.plot(whole_predict, label = \"Test set pred\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.title(\"Gangnam Whole Predict\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in tscv.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(\"\\n\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    best_weights = get_best_weights_path()\n",
    "    model[0].load_weights(best_weights)\n",
    "    \n",
    "    loaded_model = run_loaded_model(model[0], X_train, y_train, X_test, y_test, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(model, X, window_size):\n",
    "    \n",
    "    print('X shape: ', X.shape)\n",
    "    \n",
    "    trans = X[-window_size:].T\n",
    "    print(\"trans: \", trans)\n",
    "    print(\"type: \", type(trans))\n",
    "    trans.shape = (trans.size//1, 1)\n",
    "    print(\"trans: \", trans)\n",
    "    print(\"trans shape: \", trans.shape)\n",
    "    print(\"trans shape[0]: \", trans.shape[0])\n",
    "    print(\"trans shape[1]1: \", trans.shape[1])\n",
    "    \n",
    "    tmp_shape = np.reshape(trans, (trans.shape[0], 1, trans.shape[1]))\n",
    "    tmp_shape = tmp_shape.T\n",
    "    print(\"tmp_shape: \", tmp_shape)\n",
    "    pred = model.predict(tmp_shape)\n",
    "    print(\"pred: \", pred)\n",
    "    \n",
    "#     print(pred[0])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_predict = pred[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    future_pred = predict_future(loaded_model[0], whole_predict, window_size)\n",
    "    whole_predict.shape = (whole_predict.size//1, 1)\n",
    "    whole_predict = np.append(whole_predict, future_pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_plot = np.empty_like(whole_predict)\n",
    "print(len(test_predict_plot))\n",
    "print(len(whole_predict))\n",
    "\n",
    "\n",
    "print(test_predict_plot)\n",
    "test_predict_plot[:, :] = np.nan\n",
    "test_predict_plot[:, :] = whole_predict\n",
    "\n",
    "# test_predict_plot = np.insert(test_predict_plot, 0, np.nan, axis=1)\n",
    "\n",
    "test_predict_plot = np.insert(test_predict_plot, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_plot = np.insert(test_predict_plot, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_plot = np.delete(test_predict_plot, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_predict_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 5))\n",
    "plt.plot(scaler.inverse_transform(dataset), label = \"True value\")\n",
    "plt.plot(test_predict_plot, label = \"Test set pred\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Gangnam Train/test Predict\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the input data into appropriate form for Keras.\n",
    "train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
    "whole_X = np.reshape(whole_X, (whole_X.shape[0], 1, whole_X.shape[1]))\n",
    "\n",
    "print(\"New training data shape:\")\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    " \n",
    "T=25\n",
    "history_val_loss=[]\n",
    " \n",
    "for x in loaded_model[1].history['val_loss']:\n",
    "    if x >= T:\n",
    "        history_val_loss.append (T)\n",
    "    else:\n",
    "        history_val_loss.append( x )\n",
    "        \n",
    "plt.figure(figsize = (15, 15))\n",
    "plt.plot(loaded_model[1].history['loss'])\n",
    "plt.plot(history_val_loss)\n",
    "plt.ylim([0.0, 0.05])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get Best weights model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_hidden_file(fname):\n",
    "    return not fname.startswith(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_weights_path():\n",
    "    file_list = os.listdir(MODEL_PATH)\n",
    "    not_hidden_files = list(filter(ignore_hidden_file, file_list))\n",
    "    result = MODEL_PATH + \"/\" + sorted(not_hidden_files)[0]\n",
    "    print(\"Best weight path: \", result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights = get_best_weights_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loaded_model(model, train_X, train_Y, X_test,y_test, window_size = 1):\n",
    "#     adam = Adam(lr=0.00146, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=1e-6, amsgrad=False)\n",
    "    adam = Adam(lr=0.001, beta_1=0.89, beta_2=0.999, epsilon=None, decay=1e-6, amsgrad=True)\n",
    "    model.compile(loss = \"mean_squared_error\", \n",
    "                  optimizer = adam)\n",
    "    model.summary()\n",
    "    \n",
    "    checkloss = create_checkpoint(MODEL_NAME)\n",
    "    history = model.fit(train_X, train_Y, epochs = 1000 ,batch_size=4,  validation_data=(X_test, y_test),\n",
    "                        shuffle=True,\n",
    "                        callbacks=[checkloss, early_stopping])\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0].load_weights(best_weights)\n",
    "\n",
    "loaded_model = run_loaded_model(model[0], train_X, train_Y, test_X, test_Y, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(model, X, Y):\n",
    "    pred = scaler.inverse_transform(model.predict(X))\n",
    "    # 0 ~ 1 inverse origin\n",
    "    orig_data = scaler.inverse_transform([Y])\n",
    "    \n",
    "    Y, pred = np.array(Y), np.array(pred)\n",
    "    \n",
    "    return np.mean(np.abs((Y - pred) / Y) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train_score, train_predict, train_orig = predict_and_score(loaded_model[0], train_X, train_Y)\n",
    "rmse_test_score, test_predict, test_orig = predict_and_score(loaded_model[0], test_X, test_Y)\n",
    "whole_test_score, whole_predict, whole_orig = predict_and_score(loaded_model[0], whole_X, whole_Y)\n",
    "\n",
    "print(whole_X[:5])\n",
    "print(whole_Y[:5])\n",
    "print('\\n')\n",
    "print(\"Training data score: %.2f RMSE\" % rmse_train_score)\n",
    "print(\"Test data score: %.2f RMSE\" % rmse_test_score)\n",
    "print(\"Whole data score: %.2f RMSE \" % whole_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(loaded_model[0], whole_X, whole_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(whole_orig[0], whole_predict[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_predict = np.append(whole_predict, future_pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"len before: \", len(whole_predict))\n",
    "    future_pred = predict_future(model[0], whole_predict, window_size)\n",
    "    \n",
    "    print(\"future_pred\")\n",
    "    print(future_pred)\n",
    "    whole_predict = np.append(whole_predict, future_pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_predict_n_plot(dataset, train_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
